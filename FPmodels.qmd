---
title: "FPModels"
format: pdf
editor: visual
---

## Final Project Studying Latin America

# Packages

```{r}
library(tidyverse)
library(tidymodels)
library(recipes)
library(ggplot2)
library(ggrepel)
library(patchwork)
library(tidyclust)
library(dplyr)
library(themis)
library(haven)
library(rpart)
library(lubridate)
library(archive)
library(utils)
library(tigris)
library(sf)
library(geodata)
library(rnaturalearth)
library(mapview)
library(textrecipes) 
library(vip)
library(here)
library(glmnet)
library(ranger)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(yardstick)
library(rgeoboundaries)
library(readxl)
library(scales)
library(geobr)
library(spData)
library(colorspace)
library(missRanger)
library(vip)
library(ggfortify) 
library(FactoMineR) 
```

# Read in Data

```{r}
lat_am_pop_2023 <- read_dta("Merge_2023_LAPOP_AmericasBarometer_v1.0_w.dta")

Workingdata <- lat_am_pop_2023 %>% select(colorr, colori, etid, ocup4a, formal, cp13, cp8, r4a, smedia1n, q1tc_r, q2, q12cn, q10inc, edre, pais, ur, q14motan, prov)

#creating dummy for work status where 1 is working and zero is not working 
Workingdata <- Workingdata %>%
  mutate(ocup4a = case_when(
    ocup4a == 1 ~ 1,
    ocup4a == 2 ~ 0,
    ocup4a == 3 ~ 0,
    ocup4a == 4 ~ 0,
    ocup4a == 5 ~ 0,
    ocup4a == 6 ~ 0,
    ocup4a == 7 ~ 0,
  ))

#cleaning formal vs informal work 
Workingdata <- Workingdata %>%
  mutate(formal = case_when(
    formal == 1 ~ 1,
    formal == 2 ~ 0
  ))

#cleaning uses social media indicator variable
Workingdata <- Workingdata %>%
  mutate(smedia1n = case_when(
    smedia1n == 1 ~ 1,
    smedia1n == 2 ~ 0
  ))


#cleaning political involvement to be least to most 
Workingdata <- Workingdata %>%
  mutate(cp13 = case_when(
    cp13 == 4 ~ 1,
    cp13 == 3 ~ 2,
    cp13 == 2 ~ 3,
    cp13 == 1 ~ 4,
  ))

#cleaning community involvement to be least to most 
Workingdata <- Workingdata %>%
  mutate(cp8 = case_when(
    cp8 == 4 ~ 1,
    cp8 == 3 ~ 2,
    cp8 == 2 ~ 3,
    cp8 == 1 ~ 4,
  ))

#cleaning urban indicator variable
Workingdata <- Workingdata %>%
  mutate(ur = case_when(
    ur == 1 ~ 1,
    ur == 2 ~ 0
  ))

#making gender indicator variable where 0 = female, 1 = male
Workingdata <- Workingdata %>%
  mutate(q1tc_r = case_when(
    q1tc_r == 1 ~ 1,
    q1tc_r == 2 ~ 0,
    q1tc_r == 3 ~ NA,
  ))

#cleaning ethnicity (note there is no etid6, etid7 is "other" so we will convert all the additional races as "other" for simplicity)
#making black the max instead of mulata

Workingdata <- Workingdata %>%
  mutate(etid = case_when(
    etid == 1 ~ 1, 
    etid == 2 ~ 2, 
    etid == 3 ~ 3, 
    etid == 5 ~ 4, 
    etid == 4 ~ 5, 
    etid > 200 ~ 7,
    etid == 7 ~ 7, 
    TRUE ~ NA
  ))

#creating labels for ethnicity 
ethnicity_labels <- c(
  "White" = 1,
  "Mestizo" = 2,
  "Indigenous" = 3,
   "Mulata" = 4, #note that this was orignially coded as 5 and Black as 4, they have been switched 
   "Black" = 5,
  "Other" = 7
)

#creating ethnicity codes 
ethnicity_codes <- c(1, 2, 3, 4, 5, 7)

#creating skin color palette
skincolor <- c("#fff5f6", "#f5e2dc", "#e9c1b7", "#e7c9a5", "#c0a280", "#9d7c53", "#85674f", "#70503b", "#523c2f", "#422811", "#383127")


#creating skin codes 
skincodes <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)

#creating a tibble of the color pallet and skin coding 
skincolorpalette <- tibble(skincodes, skincolor)


# create indicator variable for getting to university (includes partial and completion of university)
Workingdata <- Workingdata %>%
  mutate(university = case_when(
    edre == 6 ~ 1,
    edre == 5 ~ 1,
    !(edre %in% c(5, 6)) ~ 0,
    TRUE ~ NA
  ))

#creating a factor variable for getting into university  
Workingdata <- Workingdata %>%
  mutate(university_factor = factor(case_when(
    edre == 6 ~ "University",
    edre == 5 ~ "University", 
    !(edre %in% c(5, 6)) ~ "Non-University", 
    TRUE ~ NA_character_ 
  )))

#creating labels for education 
educ_labels <- c(
  "None" = 0,
  "Primary School Incomplete" = 1,
  "Primary School Complete" = 2,
  "High-school Incomplete" = 3,
  "High-school Complete" = 4,
  "Tertiary/University Incomplete" = 5,
  "Tertiary/University Complete" = 6
)

#cleaning Income for ARGENTINA ONLY 
arg_income_codes <- c(1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715)

# Labels for ARGENITNA INCOME 
arg_income_labels <- c("Between $0 and $14.000 pesos", "Between $14.001 and $26.000 pesos", "Between $26.001 and $39.000 pesos",
            "Between $39.001 and $47.000 pesos", "Between $47.001 and $55.000 pesos", "Between $55.001 and $61.000 pesos",
            "Between $61.001 and $68.000 pesos", "Between $68.001 and $81.000 pesos", "Between $81.001 and $88.000 pesos",
            "Between $88.001 and $97.000 pesos", "Between $97.001 and $115.000 pesos", "Between $115.001 and $136.000 pesos",
            "Between $136.001 and $168.000 pesos", "Between $168.001 and $221.000 pesos", "More than $221.000 pesos")

# Assign labels FOR ARGENTINA INCOME 
arg_income_values <- factor(arg_income_codes, levels = arg_income_codes, labels = arg_income_labels)

#creating indicator variable for stating "Lack of educational opportunities" as reason for why they have considered emigrating 
Workingdata <- Workingdata %>%
  mutate(leave4educ = case_when(
    q14motan == 3 ~ 1,
    !(q14motan %in% 3) ~ 0,
    TRUE ~ NA
  ))

#Adding country labels 
Workingdata <- Workingdata %>% 
  filter(pais < 39) %>% #getting rid of US and Canada from dataset 
  mutate(country_name = factor(case_when(
    pais == 1 ~ "Mexico",
    pais == 2 ~ "Guatemala", 
    pais == 3 ~ "El Salvador",
    pais == 4 ~ "Honduras", 
    pais == 5 ~ "Nicaragua",
    pais == 6 ~ "Costa Rica", 
    pais == 7 ~ "Panama",
    pais == 8 ~ "Colombia", 
    pais == 9 ~ "Ecuador",
    pais == 10 ~ "Bolivia", 
    pais == 11 ~ "Peru", 
    pais == 12 ~ "Paraguay",
    pais == 13 ~ "Chile", 
    pais == 14 ~ "Uruguay",
    pais == 15 ~ "Brazil", 
    pais == 16 ~ "Venezuela",
    pais == 17 ~ "Argentina", 
    pais == 21 ~ "Dominican Republic", 
    pais == 22 ~ "Haiti", 
    pais == 23 ~ "Jamaica",
    pais == 24 ~ "Guyana", 
    pais == 25 ~ "Trinidad & Tobago",     
    pais == 26 ~ "Belize",
    pais == 27 ~ "Suriname", 
    pais == 28 ~ "Bahamas", 
    pais == 29 ~ "Barbados",
    pais == 30 ~ "Grenada", 
    pais == 31 ~ "Saint Lucia", 
    pais == 32 ~ "Dominica", 
    pais == 33 ~ "Antigua and Barbuda", 
    pais == 34 ~ "Saint Vincent and the Grenadines", 
    pais == 35 ~ "Saint Kitts and Nevis",
    TRUE ~ NA_character_ 
  ))) %>%
  relocate(country_name, .after = pais) 

Workingdata <- Workingdata %>%
  rename(skin_color = colorr,
         ethnicity = etid,
         employed = ocup4a,
         mobile = r4a, 
         social_media = smedia1n,
         male = q1tc_r,
         age = q2, 
         income = q10inc,
         educ_attainment = edre,
         political_involvement = cp13,
         community_involvement  =cp8, 
         country_num = pais,
         household_members = q12cn)


SouthAmerica_only <- Workingdata %>% 
  filter(country_name == "Argentina" 
         | country_name == "Uruguay" 
         | country_name == "Chile" 
         | country_name == "Paraguay" 
         | country_name == "Bolivia" 
         | country_name == "Peru" 
         | country_name == "Ecuador" 
         | country_name == "Brazil" 
         | country_name == "Suriname" 
         | country_name == "Guyana" 
         | country_name == "French Guiana"  
         | country_name == "Venezuela" 
         | country_name == "Colombia")


```


```{r}
# Check for missing values
missing_values <- sum(is.na(Workingdata))
if (missing_values > 0) {
  cat("There are", missing_values, "missing values in the dataset.\n")
  # Handle missing values here (e.g., imputation or other preprocessing steps)
} else {
  cat("No missing values found in the dataset.\n")
}

#There are 122946 missing values in the dataset

#Imputing missing values with MissRanger
Workingdata <- missRanger(Workingdata, num.trees = 30, num.threads = 2)
summary(Workingdata)

#No Missing values found in the dataset

```
We use Library missRanger for imputing missing values. The "ranger" package (Wright & Ziegler) is helpful to do fast missing value imputation by chained random forests, see Stekhoven & Buehlmann and Van Buuren & Groothuis-Oudshoorn. Between the iterative model fitting, it offers the option of predictive mean matching. This firstly avoids imputation with values not present in the original data (like a value 0.3334 in a 0-1 coded variable). Secondly, predictive mean matching tries to raise the variance in the resulting conditional distributions to a realistic level. This allows to do multiple imputation when repeating the call to missRanger().

References
1. Wright, M. N. & Ziegler, A. (2016). ranger: A Fast Implementation of Random Forests for
High Dimensional Data in C++ and R. Journal of Statistical Software, in press. <arxiv.org/abs/1508.04409>.
2. Stekhoven, D.J. and Buehlmann, P. (2012). ’MissForest - nonparametric missing value impu-
tation for mixed-type data’, Bioinformatics, 28(1) 2012, 112-118. https://doi.org/10.1093/bioinformatics/btr597.
3. Van Buuren, S., Groothuis-Oudshoorn, K. (2011). mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software, 45(3), 1-67. http://www.jstatsoft.org/v45/i03/


## 5. Data Analysis: using machine learning to predict attending to university in Argentina

In this section, we focus on understanding the determinants of achieving university education in Argentina. A key feature of the Argentine superior education system is that the country has a public university system funded by the federal government to be "free" for students.

## 5.1 Unsupervised Machine Learning

```{r}
Argentinadata <- Workingdata %>% filter(country_num==17) #Choosing Argentina

Argentinadata <- Argentinadata %>%  select(-country_name, -country_num, -university_factor)

```

### Cluster Analysis 
```{r}
set.seed(20201020)
num_clusters <- 4

# Fit the k-means clustering model
kmeans_model <- kmeans(Argentinadata, centers = num_clusters, nstart = 100)

# Print the kmeans_model object
kmeans_model

# Retrieve cluster centers
cluster_centers <- kmeans_model$centers

# Assign cluster labels to each observation
cluster_labels <- kmeans_model$cluster

# Add cluster labels to the original data
Argentinadata_with_clusters <- cbind(Argentinadata, Cluster = cluster_labels)

tidy(kmeans_model) %>%
  knitr::kable(digits = 2)

# Plot the clusters
plot(Argentinadata_with_clusters[, c("age", "educ_attainment")], 
     col = cluster_labels, pch = 19, main = "K-means Clustering", 
     xlab = "Age", ylab = "Education Attainment")


```

```{r}
# Perform PCA analysis
pca_result <- PCA(Argentinadata, graph = FALSE)

# Plot the PCA results
pca_with_clusters <- cbind(as.data.frame(pca_result$ind$coord), Cluster = cluster_labels)

# Plot PCA results with cluster labels
ggplot(pca_with_clusters, aes(x = Dim.1, y = Dim.2, color = factor(Cluster))) +
  geom_point() +
  labs(title = "PCA with K-means Clustering", x = "PC1", y = "PC2", color = "Cluster") +
  theme_minimal()
```



## 5.2 Supervised Machine Learning
```{r}
Argentinadata <- Workingdata %>% filter(country_num==17) #Choosing Argentina

Argentinadata <- Argentinadata %>% select(skin_color, ethnicity, employed, formal, political_involvement, community_involvement, mobile, social_media, male, age, household_members, income, university_factor)

#Splitting the data
set.seed(20201020)
# create a split object
educ_split <- initial_split(data = Argentinadata, prop = 0.75)
# create the training and testing data
workingdata_train <- training(x = educ_split)
workingdata_test <- testing(x = educ_split)
```

## Come up with Models & Estimation

Building a **probit classififation model** with embedded regularization to quantify likelihood of making it to college based on our pre-defined predictors:

```{r}
# Create a recipe
probit_recipe <- recipe(formula = university_factor ~ ., data = workingdata_train) %>%
    step_normalize(all_numeric_predictors()) %>%
    step_nzv(all_numeric_predictors())
             
# Set Probit Model
probit_model <- logistic_reg() %>%
  set_engine("glm", family = binomial(link = "probit"))

# Probit Workflow and recipe
probit_wf <- workflow() %>%
  add_model(probit_model) %>%
  add_recipe(probit_recipe)

# Set folds for cross-validation
folds <- vfold_cv(data = workingdata_train, v = 10)

set.seed(20201020)
# Fitting probit model using v-fold resampling method
probit_fit_rs <- probit_wf %>%
  fit_resamples(
    resamples = folds,
    control = control_resamples(save_pred = TRUE, save_workflow = TRUE),
    metrics = metric_set(accuracy, precision, recall)
  )
```

Building a **decision tree** to quantify likelihood of making it to college based on our pre-defined predictors:

```{r}
set.seed(20201020)

# Create the recipe object 
tree_recipe <- recipe(formula = university_factor ~ ., data = workingdata_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) 

# Define the parameter grid
param_grid <- expand.grid(
  min_n = c(2, 5, 10, 15),   
  max_depth = c(2, 3, 4)
)

# Create a cart model object
lapop_tree <- 
  decision_tree() %>%
  set_engine("rpart", model = TRUE) %>%
  set_mode("classification")  

# Create a workflow
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(lapop_tree) 

# Tune the model
tree_tune <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    control = control_grid(verbose = TRUE),
    metrics = metric_set(accuracy, precision, recall)
  )

# Extract the best tuning parameters
best_params <- tree_tune %>%
  select_best(metric = "accuracy")

# Create a new tree model with the best parameters
best_tree_model <- lapop_tree %>%
  set_engine("rpart", model = TRUE) %>%
  set_mode("classification") %>%
  finalize_model(best_params)

# Fit the tuned model
best_tree_fit <- best_tree_model %>%
  fit(data = workingdata_train, formula = university_factor ~ .)

# create a tree
rpart.plot::rpart.plot(x = best_tree_fit$fit)

```

Building a **random forest** to quantify likelihood of making it to college based on our pre-defined predictors:

```{r}
set.seed(20201020)

# Create the recipe object and impute missing values
rf_recipe <- recipe(formula = university_factor ~ ., data = workingdata_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors())

# Create a random forest model object
lapop_rf <- 
  rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

# Create a workflow
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(lapop_rf)

# Fit the model
rf_fit_rs <- rf_wf %>%
  fit_resamples(
    resamples = folds,
    control = control_resamples(save_pred = TRUE, save_workflow = TRUE),
    metrics = metric_set(precision, accuracy, recall)
  )

```

Evaluate the models:

```{r}
#Collecing metrics
probit_metrics <- collect_metrics(probit_fit_rs, summarize = FALSE)
tree_metrics <- collect_metrics(tree_tune, summarize = FALSE)
rf_metrics <- collect_metrics(rf_fit_rs, summarize = FALSE)

# Add a column to indicate the model for each metrics dataframe
probit_metrics$model <- "Probit"
tree_metrics$model <- "Decision Tree"
rf_metrics$model <- "Random Forest"

# Combine the model metrics into one data frame
combined_metrics <- bind_rows(probit_metrics, tree_metrics, rf_metrics)

#Evaluate
combined_metrics %>%
  group_by(model, .metric) %>%
  mutate(modelmean = mean(.estimate)) %>%
  ungroup() %>%
  ggplot(aes(x = model, color = .metric)) + 
  geom_jitter(aes(y = .estimate), 
              position = position_dodge(width = 0.5), 
              alpha = 0.65, 
              size = 2) +
  geom_point(aes(y = modelmean), 
             shape = 95, 
             position = position_dodge(width = 0.5), 
             size = 10) +
  labs(x = "Model", 
       y = "Metric", 
       title = "Model performance") +
  theme_minimal()
```
**Decision Tree Model**

-   **Accuracy**: The Decision Tree model's accuracy exhibits some variability, particularly towards the lower end, but it generally remains good, consistently above 70%. Despite some fluctuations, it maintains a respectable level of correctness in its predictions across different evaluations.

-   **Precision**: Precision of the Decision Tree model varies across evaluations, performing poorer than Random Forest but better than Probit on average. While it might not achieve the same level of precision as Random Forest, it still manages to provide relatively accurate positive predictions, albeit with some variability.

-   **Recall**: Decision Tree model demonstrates the highest recall compared to the other models, on average. It excels in capturing positive instances, even though its recall values might fluctuate significantly across different evaluations. Despite the variability, its average recall tends to be higher than both Random Forest and Probit, indicating its effectiveness in identifying relevant cases within the dataset.

**Random Forest Model**

-   **Accuracy**: The Random Forest model generally exhibits higher accuracy with less variability across different evaluations. This means it consistently provides accurate predictions across various datasets or subsets. It is above 70%.

-   **Precision**: Additionally, Random Forest tends to have higher precision compared to other models, indicating fewer false positives. Moreover, it shows less variability in precision outcomes across different folds or evaluations, suggesting more stable performance.

-   **Recall**: While Random Forest tends to have good overall performance, its recall might not be as high as some other models on average.

**Probit Model**:

-   **Accuracy**: The Probit model demonstrates considerable variability in accuracy across different estimations. While in some cases, it may outperform both Random Forest and Decision Trees, in others, its accuracy can be significantly lower. This inconsistency suggests that the Probit model's performance is less stable in terms of overall correctness of predictions. It is around 70%.

-   **Precision**: Precision of the Probit model tends to exhibit high variability and is generally poorer on average compared to other models. This indicates that the Probit model might struggle to effectively minimize false positives, leading to a higher proportion of incorrect positive predictions compared to Random Forest and Decision Trees.

-   **Recall**: The Probit model's recall performance is not as strong as other models. It might fail to capture as many positive instances as Random Forest or Decision Trees, indicating limitations in its ability to identify all relevant cases within the dataset.

In a nutshell, the Random Forest model performed better in all the pre-selected evaluation metrics. In the nex section, we test it against out testing data. 


## Variable importance in the random forest model

```{r}
# Fit the random forest model
library(randomForest)

rf_model <- randomForest(formula = university_factor ~ ., data = workingdata_train, ntree = 500)

# Calculate variable importance
var_importance <- importance(rf_model)

# Plot variable importance
var_importance_plot <- varImpPlot(rf_model, main = "Variable Importance - Random Forest")
print(var_importance_plot)

```

## 3. Implementation of a Random Forest

```{r}
# Preprocess the test dataset using the recipe
test_processed <- rf_recipe %>%
  prep() %>%
  bake(new_data = workingdata_test)

# Fit the model and select the best tuning configuration
rf_fit_best <- rf_fit_rs %>%
  fit_best()

# Predict using the best model configuration
test_predictions <- rf_fit_best %>%
  predict(new_data = workingdata_test) %>%
  bind_cols(workingdata_test)

# Evaluate the model's performance on the test dataset
test_metrics <- test_predictions %>%
  metrics(truth = university_factor, estimate = .pred_class)

# View the test metrics
print(test_metrics)
```

